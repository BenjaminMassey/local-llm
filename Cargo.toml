[package]
name = "local-llm"
version = "0.1.0"
edition = "2021"
authors = ["Benjamin Massey <benjamin.w.massey@gmail.com>"]
description = "A simplified wrapper around the \"llama_cpp_rs\" crate for local usage of a Llama LLM."
license = "MIT"
repository = "https://github.com/BenjaminMassey/local-llm"
keywords = ["ai", "llm", "gpt", "llama", "chat"]

[dependencies]
gag = "1.0.0"
llama_cpp_rs = { git = "https://github.com/mdrokz/rust-llama.cpp", rev = "3377bfd" }

[features]
cuda = ["llama_cpp_rs/cuda"]

[dev-dependencies]
version-sync = "0.9"

[badges.maintenance]
status = "actively-developed"